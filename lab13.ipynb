{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to text processing using NLTK\n",
    "The Natural Language Toolkit (NLTK) is a powerful Python package that\n",
    "provides a set of diverse natural languages algorithms. It is free, open source,\n",
    "easy to use, large community, and well documented. NLTK consists of the\n",
    "most common algorithms such as tokenizing, part-of-speech tagging, stemming,\n",
    "1\n",
    "sentiment analysis, topic segmentation, and named entity recognition.\n",
    "In this tutorial, you are going to cover the following topics:\n",
    "- Text Analysis Operations using NLTK\n",
    "- Tokenization\n",
    "- Stopwords\n",
    "- Lexicon Normalization such as Stemming and Lemmatization\n",
    "- POS Tagging\n",
    "\n",
    "Those are common practices in Natural Language Processing (NLP)\n",
    "___\n",
    "#### NLTK installation\n",
    "The first step is to install NLTK. You can run the Anaconda Prompt. Type `conda install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new window should open, showing the `NLTK Downloader`. You don’t have\n",
    "to download all packages. You need to download `Popular packages` by downloading popular package in the Collections\n",
    "tab.\n",
    "\n",
    "### Tokenization\n",
    "The process of breaking down a\n",
    "text paragraph into smaller chunks such as words or sentence is called Tokenization.\n",
    "\n",
    "#### Sentence Tokenization\n",
    "Sentence tokenizer breaks text paragraph into sentences. Use the following code\n",
    "to use NLTK for sentence tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"Dogs ,Canis lupus familiaris, are domesticated mammals, not natural wild animals. They were originally bred from\n",
    "wolves. They have been bred by humans for a long time, and\n",
    "were the first animals ever to be domesticated.\n",
    "Today, some dogs are used as pets, others are used to help\n",
    "humans do their work. They are a popular pet because they\n",
    "are usually playful, friendly, loyal and listen to humans.\n",
    "Thirty million dogs in the United States are registered as\n",
    "pets. Dogs eat both meat and vegetables, often mixed\n",
    "together and sold in stores as dog food. Dogs often have\n",
    "jobs, including as police dogs, army dogs, assistance dogs,\n",
    "fire dogs, messenger dogs, hunting dogs, herding dogs, or\n",
    "rescue dogs.\n",
    "They are sometimes called \"canines\" from the Latin word for dog\n",
    "- canis. Sometimes people also use \"dog\" to describe other\n",
    "canids, such as wolves. A baby dog is called a pup or puppy.\n",
    "A dog is called a puppy until it is about one year old.\n",
    "Dogs are sometimes referred to as \"man’s best friend\" because\n",
    "they are kept as domestic pets and are usually loyal and\n",
    "like being around humans.\n",
    "I am ICT student \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Dogs ,Canis lupus familiaris, are domesticated mammals, not natural wild animals.', 'They were originally bred from\\nwolves.', 'They have been bred by humans for a long time, and\\nwere the first animals ever to be domesticated.', 'Today, some dogs are used as pets, others are used to help\\nhumans do their work.', 'They are a popular pet because they\\nare usually playful, friendly, loyal and listen to humans.', 'Thirty million dogs in the United States are registered as\\npets.', 'Dogs eat both meat and vegetables, often mixed\\ntogether and sold in stores as dog food.', 'Dogs often have\\njobs, including as police dogs, army dogs, assistance dogs,\\nfire dogs, messenger dogs, hunting dogs, herding dogs, or\\nrescue dogs.', 'They are sometimes called \"canines\" from the Latin word for dog\\n- canis.', 'Sometimes people also use \"dog\" to describe other\\ncanids, such as wolves.', 'A baby dog is called a pup or puppy.', 'A dog is called a puppy until it is about one year old.', 'Dogs are sometimes referred to as \"man’s best friend\" because\\nthey are kept as domestic pets and are usually loyal and\\nlike being around humans.', 'I am ICT student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(text)    # sentence tokenization\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization\n",
    "Word tokenizer breaks text paragraph into words. Use the following code to use\n",
    "NLTK for word tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Dogs', ',', 'Canis', 'lupus', 'familiaris', ',', 'are', 'domesticated', 'mammals', ',', 'not', 'natural', 'wild', 'animals', '.', 'They', 'were', 'originally', 'bred', 'from', 'wolves', '.', 'They', 'have', 'been', 'bred', 'by', 'humans', 'for', 'a', 'long', 'time', ',', 'and', 'were', 'the', 'first', 'animals', 'ever', 'to', 'be', 'domesticated', '.', 'Today', ',', 'some', 'dogs', 'are', 'used', 'as', 'pets', ',', 'others', 'are', 'used', 'to', 'help', 'humans', 'do', 'their', 'work', '.', 'They', 'are', 'a', 'popular', 'pet', 'because', 'they', 'are', 'usually', 'playful', ',', 'friendly', ',', 'loyal', 'and', 'listen', 'to', 'humans', '.', 'Thirty', 'million', 'dogs', 'in', 'the', 'United', 'States', 'are', 'registered', 'as', 'pets', '.', 'Dogs', 'eat', 'both', 'meat', 'and', 'vegetables', ',', 'often', 'mixed', 'together', 'and', 'sold', 'in', 'stores', 'as', 'dog', 'food', '.', 'Dogs', 'often', 'have', 'jobs', ',', 'including', 'as', 'police', 'dogs', ',', 'army', 'dogs', ',', 'assistance', 'dogs', ',', 'fire', 'dogs', ',', 'messenger', 'dogs', ',', 'hunting', 'dogs', ',', 'herding', 'dogs', ',', 'or', 'rescue', 'dogs', '.', 'They', 'are', 'sometimes', 'called', '``', 'canines', \"''\", 'from', 'the', 'Latin', 'word', 'for', 'dog', '-', 'canis', '.', 'Sometimes', 'people', 'also', 'use', '``', 'dog', \"''\", 'to', 'describe', 'other', 'canids', ',', 'such', 'as', 'wolves', '.', 'A', 'baby', 'dog', 'is', 'called', 'a', 'pup', 'or', 'puppy', '.', 'A', 'dog', 'is', 'called', 'a', 'puppy', 'until', 'it', 'is', 'about', 'one', 'year', 'old', '.', 'Dogs', 'are', 'sometimes', 'referred', 'to', 'as', '``', 'man', '’', 's', 'best', 'friend', \"''\", 'because', 'they', 'are', 'kept', 'as', 'domestic', 'pets', 'and', 'are', 'usually', 'loyal', 'and', 'like', 'being', 'around', 'humans', '.', 'I', 'am', 'ICT', 'student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text)    # word tokenization\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution\n",
    "Let’s calculate the frequency distribution of those words using Python NLTK.\n",
    "There is a function in NLTK called `FreqDist()` that does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 124 samples and 234 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18),\n",
       " ('.', 13),\n",
       " ('are', 10),\n",
       " ('dogs', 10),\n",
       " ('as', 7),\n",
       " ('and', 6),\n",
       " ('to', 5),\n",
       " ('dog', 5),\n",
       " ('``', 4),\n",
       " ('Dogs', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)   # top 10 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "Any piece of text which is not relevant to the context of the data and the\n",
    "end-output can be specified as the noise. For example – language stopwords\n",
    "(commonly used words of a language – is, am, the, of, in etc), URLs or links,\n",
    "social media entities (mentions, hashtags), punctuations and industry specific\n",
    "words.\n",
    "\n",
    "This step deals with removal of all types of noisy entities present in the\n",
    "text. In NLTK for removing stopwords, you need to create a list of stopwords and\n",
    "filter out your list of tokens from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = ['is', 'a', 'this', ',', 'ICT']   # create a list of stop word that you want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# NLTK also provide a set of common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'do', 'as', 'same', 'further', 'my', 'me', 'aren', 'up', 'weren', 't', 'couldn', 'have', 'you', 'and', 'about', 'all', 'against', \"won't\", 'now', 'should', 'his', 'hasn', \"mustn't\", 'to', 'each', 'who', 'ain', 'haven', 'while', \"shan't\", 'at', \"don't\", 'off', 've', \"couldn't\", \"it's\", 'your', 'won', \"you'd\", 'him', 'very', 'before', 'any', 'doesn', 'itself', 'between', 'why', \"haven't\", 'both', 'when', 'we', 'here', 'own', \"isn't\", 'because', 'with', 'of', 'being', 'most', \"hadn't\", 'below', 'our', \"that'll\", 'll', 'from', \"you've\", 'down', 'in', 'some', \"she's\", 'only', 'hers', 'having', 'if', 'does', 'don', 'what', 'again', 'ours', 'or', 'just', 'ma', 'too', 'but', 'her', 'yourself', 'these', 'nor', 'for', 'those', 'myself', 'so', 'which', 'into', 'it', 'shouldn', 'this', 'more', \"you're\", 'can', 'after', 'had', 'during', 'is', 's', 'y', 'whom', 'how', 'was', 'am', 'yours', 'under', 'wouldn', 'above', 'theirs', \"didn't\", \"should've\", 'by', 'their', 'a', 'isn', 'no', 'mightn', 'mustn', \"aren't\", 'hadn', 'were', 'on', 'did', 'once', 'not', 'themselves', 'an', 'there', 'has', 'its', 'until', 'be', 'then', \"needn't\", \"shouldn't\", \"hasn't\", 'through', 'them', \"doesn't\", 'i', 'where', \"weren't\", 'will', 'he', 'o', 'didn', 're', 'the', 'been', 'few', 'they', \"mightn't\", 'she', 'over', 'd', 'herself', 'that', 'are', 'out', 'such', 'm', 'ourselves', 'shan', 'doing', \"wasn't\", 'wasn', 'other', \"you'll\", 'than', \"wouldn't\", 'yourselves', 'needn'}\n"
     ]
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords\n",
    "Next, you are going to use the stopwords that you prepared to remove noise\n",
    "from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['``', 'Dogs', 'Canis', 'lupus', 'familiaris', 'are', 'domesticated', 'mammals', 'not', 'natural', 'wild', 'animals', '.', 'They', 'were', 'originally', 'bred', 'from', 'wolves', '.', 'They', 'have', 'been', 'bred', 'by', 'humans', 'for', 'long', 'time', 'and', 'were', 'the', 'first', 'animals', 'ever', 'to', 'be', 'domesticated', '.', 'Today', 'some', 'dogs', 'are', 'used', 'as', 'pets', 'others', 'are', 'used', 'to', 'help', 'humans', 'do', 'their', 'work', '.', 'They', 'are', 'popular', 'pet', 'because', 'they', 'are', 'usually', 'playful', 'friendly', 'loyal', 'and', 'listen', 'to', 'humans', '.', 'Thirty', 'million', 'dogs', 'in', 'the', 'United', 'States', 'are', 'registered', 'as', 'pets', '.', 'Dogs', 'eat', 'both', 'meat', 'and', 'vegetables', 'often', 'mixed', 'together', 'and', 'sold', 'in', 'stores', 'as', 'dog', 'food', '.', 'Dogs', 'often', 'have', 'jobs', 'including', 'as', 'police', 'dogs', 'army', 'dogs', 'assistance', 'dogs', 'fire', 'dogs', 'messenger', 'dogs', 'hunting', 'dogs', 'herding', 'dogs', 'or', 'rescue', 'dogs', '.', 'They', 'are', 'sometimes', 'called', '``', 'canines', \"''\", 'from', 'the', 'Latin', 'word', 'for', 'dog', '-', 'canis', '.', 'Sometimes', 'people', 'also', 'use', '``', 'dog', \"''\", 'to', 'describe', 'other', 'canids', 'such', 'as', 'wolves', '.', 'A', 'baby', 'dog', 'called', 'pup', 'or', 'puppy', '.', 'A', 'dog', 'called', 'puppy', 'until', 'it', 'about', 'one', 'year', 'old', '.', 'Dogs', 'are', 'sometimes', 'referred', 'to', 'as', '``', 'man', '’', 's', 'best', 'friend', \"''\", 'because', 'they', 'are', 'kept', 'as', 'domestic', 'pets', 'and', 'are', 'usually', 'loyal', 'and', 'like', 'being', 'around', 'humans', '.', 'I', 'am', 'student']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = []\n",
    "for w in tokenized_word:\n",
    "    if w not in my_stopwords:\n",
    "        filtered_words.append(w)\n",
    "        \n",
    "# print(\"Tokenized Sentence:\", tokenized_word)\n",
    "# print()\n",
    "print(\"Filtered Sentence:\",filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence: ['``', 'Dogs', ',', 'Canis', 'lupus', 'familiaris', ',', 'domesticated', 'mammals', ',', 'natural', 'wild', 'animals', '.', 'They', 'originally', 'bred', 'wolves', '.', 'They', 'bred', 'humans', 'long', 'time', ',', 'first', 'animals', 'ever', 'domesticated', '.', 'Today', ',', 'dogs', 'used', 'pets', ',', 'others', 'used', 'help', 'humans', 'work', '.', 'They', 'popular', 'pet', 'usually', 'playful', ',', 'friendly', ',', 'loyal', 'listen', 'humans', '.', 'Thirty', 'million', 'dogs', 'United', 'States', 'registered', 'pets', '.', 'Dogs', 'eat', 'meat', 'vegetables', ',', 'often', 'mixed', 'together', 'sold', 'stores', 'dog', 'food', '.', 'Dogs', 'often', 'jobs', ',', 'including', 'police', 'dogs', ',', 'army', 'dogs', ',', 'assistance', 'dogs', ',', 'fire', 'dogs', ',', 'messenger', 'dogs', ',', 'hunting', 'dogs', ',', 'herding', 'dogs', ',', 'rescue', 'dogs', '.', 'They', 'sometimes', 'called', '``', 'canines', \"''\", 'Latin', 'word', 'dog', '-', 'canis', '.', 'Sometimes', 'people', 'also', 'use', '``', 'dog', \"''\", 'describe', 'canids', ',', 'wolves', '.', 'A', 'baby', 'dog', 'called', 'pup', 'puppy', '.', 'A', 'dog', 'called', 'puppy', 'one', 'year', 'old', '.', 'Dogs', 'sometimes', 'referred', '``', 'man', '’', 'best', 'friend', \"''\", 'kept', 'domestic', 'pets', 'usually', 'loyal', 'like', 'around', 'humans', '.', 'I', 'ICT', 'student']\n"
     ]
    }
   ],
   "source": [
    "NLTK_filtered_words = []\n",
    "for w in tokenized_word:\n",
    "    if w not in nltk_stop_words:\n",
    "        NLTK_filtered_words.append(w)\n",
    "\n",
    "print(\"Filtered sentence:\", NLTK_filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "Lexicon normalization considers another type of noise in the text. For example,\n",
    "connection, connected, connecting word reduce to a common word “connect”.\n",
    "It reduces derivationally related forms of a word to a common root word.\n",
    "The most common lexicon normalization practices are\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "#### Stemming\n",
    "Stemming is a process of linguistic normalization, which reduces words to their\n",
    "word root word or chops off the derivational affixes (“ing”, “ly”, “es”, “s” etc).\n",
    "For example, connection, connected, connecting word reduce to a common word\n",
    "“connect”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'commun'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "word=\"communication\"\n",
    "p.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: ['``', 'dog', 'cani', 'lupu', 'familiari', 'are', 'domest', 'mammal', 'not', 'natur', 'wild', 'anim', '.', 'they', 'were', 'origin', 'bred', 'from', 'wolv', '.', 'they', 'have', 'been', 'bred', 'by', 'human', 'for', 'long', 'time', 'and', 'were', 'the', 'first', 'anim', 'ever', 'to', 'be', 'domest', '.', 'today', 'some', 'dog', 'are', 'use', 'as', 'pet', 'other', 'are', 'use', 'to', 'help', 'human', 'do', 'their', 'work', '.', 'they', 'are', 'popular', 'pet', 'becaus', 'they', 'are', 'usual', 'play', 'friendli', 'loyal', 'and', 'listen', 'to', 'human', '.', 'thirti', 'million', 'dog', 'in', 'the', 'unit', 'state', 'are', 'regist', 'as', 'pet', '.', 'dog', 'eat', 'both', 'meat', 'and', 'veget', 'often', 'mix', 'togeth', 'and', 'sold', 'in', 'store', 'as', 'dog', 'food', '.', 'dog', 'often', 'have', 'job', 'includ', 'as', 'polic', 'dog', 'armi', 'dog', 'assist', 'dog', 'fire', 'dog', 'messeng', 'dog', 'hunt', 'dog', 'herd', 'dog', 'or', 'rescu', 'dog', '.', 'they', 'are', 'sometim', 'call', '``', 'canin', \"''\", 'from', 'the', 'latin', 'word', 'for', 'dog', '-', 'cani', '.', 'sometim', 'peopl', 'also', 'use', '``', 'dog', \"''\", 'to', 'describ', 'other', 'canid', 'such', 'as', 'wolv', '.', 'A', 'babi', 'dog', 'call', 'pup', 'or', 'puppi', '.', 'A', 'dog', 'call', 'puppi', 'until', 'it', 'about', 'one', 'year', 'old', '.', 'dog', 'are', 'sometim', 'refer', 'to', 'as', '``', 'man', '’', 's', 'best', 'friend', \"''\", 'becaus', 'they', 'are', 'kept', 'as', 'domest', 'pet', 'and', 'are', 'usual', 'loyal', 'and', 'like', 'be', 'around', 'human', '.', 'I', 'am', 'student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_words:\n",
    "    stemmed_words.append(stem.stem(w))\n",
    "    \n",
    "# print(\"Filtered Sentence:\", filtered_words)\n",
    "# print()\n",
    "print(\"Stemmed Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization, on the other hand, is an organized and step by step procedure\n",
    "of obtaining the root form of the word, it makes use of vocabulary (dictionary\n",
    "importance of words) and morphological analysis (word structure and grammar\n",
    "relations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize(\"walking\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly\n",
      "flying\n"
     ]
    }
   ],
   "source": [
    "word = \"flying\"\n",
    "print(lem.lemmatize(word, pos='v'))\n",
    "print(lem.lemmatize(word, pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical\n",
    "group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE,\n",
    "VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships\n",
    "within the sentence and assigns a corresponding tag to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rin', 'likes', 'to', 'eat', 'chocolate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Rin', 'NNP'),\n",
       " ('likes', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('eat', 'VB'),\n",
       " ('chocolate', 'NN')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Rin likes to eat chocolate\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)\n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
